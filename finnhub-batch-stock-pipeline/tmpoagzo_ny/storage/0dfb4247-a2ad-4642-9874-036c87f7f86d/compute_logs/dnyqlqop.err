[32m2024-06-15 13:42:47 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 166558 - LOGS_CAPTURED - Started capturing logs in process (pid: 166558).
[32m2024-06-15 13:42:48 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 166558 - spark_operator.create_stock_tables[stock_AACS] - STEP_START - Started execution of step "spark_operator.create_stock_tables[stock_AACS]".
[32m2024-06-15 13:42:48 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - spark_operator.create_stock_tables[stock_AACS] - Loading file from: /workspaces/practical-data-engineering/finnhub-batch-stock-pipeline/tmpoagzo_ny/storage/0dfb4247-a2ad-4642-9874-036c87f7f86d/spark_operator.stock_tables/result/stock_AACS using PickledObjectFilesystemIOManager...
[32m2024-06-15 13:42:48 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 166558 - spark_operator.create_stock_tables[stock_AACS] - LOADED_INPUT - Loaded input "input_fn" using input manager "io_manager", from output "result" of step "spark_operator.stock_tables"
[32m2024-06-15 13:42:48 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 166558 - spark_operator.create_stock_tables[stock_AACS] - STEP_INPUT - Got input "input_fn" of type "Any". (Type check passed).
24/06/15 13:42:57 WARN Utils: Your hostname, codespaces-e7c8c2 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)
24/06/15 13:42:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
24/06/15 13:42:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/06/15 13:43:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
24/06/15 13:43:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
24/06/15 13:43:05 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
24/06/15 13:43:05 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
24/06/15 13:43:17 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors
[Stage 0:>                                                          (0 + 0) / 4][Stage 0:>                                                          (0 + 1) / 4][Stage 0:>                                                          (0 + 4) / 4][Stage 0:=============================>                             (2 + 2) / 4]                                                                                24/06/15 13:43:46 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 1:>                                                          (0 + 4) / 4][Stage 1:============================================>              (3 + 1) / 4][Stage 3:>                                                          (0 + 1) / 1]                                                                                [Stage 4:>                                                          (0 + 1) / 1]                                                                                [Stage 5:>                                                          (0 + 3) / 3][Stage 5:===================>                                       (1 + 2) / 3]                                                                                [32m2024-06-15 13:43:58 +0000[0m - dagster - [34mINFO[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - spark_operator.create_stock_tables[stock_AACS] - Row(10DayAverageTradingVolume=0.02513, 3MonthAverageTradingVolume=0.02513, 52WeekHigh=0.0001, 52WeekHighDate='2023-08-11', 52WeekLow=1e-06, 52WeekLowDate='2024-04-16', 5DayPriceReturnDaily=0, assetTurnoverAnnual=0.4314, assetTurnoverTTM=0.4274, beta=-45.643253, bookValuePerShareAnnual=0.0013, bookValuePerShareQuarterly=0.0013, bookValueShareGrowth5Y=-1.47, capexCagr5Y=37.02, cashFlowPerShareAnnual=0, cashFlowPerShareQuarterly=0, cashPerSharePerShareAnnual=0, cashPerSharePerShareQuarterly=0, currentRatioAnnual=2.0199, currentRatioQuarterly=2.0199, ebitdPerShareAnnual=0, ebitdPerShareTTM=0, ebitdaCagr5Y=None, ebitdaInterimCagr5Y=None, enterpriseValue=0.9157, epsAnnual=-0.0002, epsBasicExclExtraItemsAnnual=-0.0002, epsBasicExclExtraItemsTTM=-0.0002, epsExclExtraItemsAnnual=-0.0002, epsExclExtraItemsTTM=-0.0002, epsGrowth3Y=None, epsGrowth5Y=None, epsGrowthQuarterlyYoy=None, epsGrowthTTMYoy=None, epsInclExtraItemsAnnual=-0.0002, epsInclExtraItemsTTM=-0.0002, epsNormalizedAnnual=-0.0002, epsTTM=-0.0002, focfCagr5Y=None, grossMargin5Y=52.88, grossMarginAnnual=51.28, grossMarginTTM=49.12, inventoryTurnoverAnnual=4.1718, inventoryTurnoverTTM=4.1718, longTermDebt/equityAnnual=0.2016, longTermDebt/equityQuarterly=0.2016, marketCapitalization=0.00115781, monthToDatePriceReturnDaily=0, netInterestCoverageAnnual=-3.841, netInterestCoverageTTM=-3.8418, netMarginGrowth5Y=None, netProfitMargin5Y=-5.14, netProfitMarginAnnual=-11.91, netProfitMarginTTM=-14.36, operatingMargin5Y=-1.71, operatingMarginAnnual=-9.69, operatingMarginTTM=-12.35, pbAnnual=1.7973, pbQuarterly=1.7973, pcfShareAnnual=None, pcfShareTTM=None, peAnnual=None, peBasicExclExtraTTM=None, peExclExtraTTM=None, peInclExtraTTM=None, peNormalizedAnnual=None, peTTM=None, pfcfShareAnnual=0.0053, pfcfShareTTM=0.0075, pretaxMargin5Y=-5.14, pretaxMarginAnnual=-11.91, pretaxMarginTTM=-14.36, priceRelativeToS&P500Ytd=2.0162, psAnnual=0.0006, psTTM=0.0006, ptbvAnnual=0.3633, ptbvQuarterly=10.1627, quickRatioAnnual=1.8441, quickRatioQuarterly=1.8441, receivablesTurnoverAnnual=2.0801, receivablesTurnoverTTM=2.0801, revenueGrowth3Y=-4.42, revenueGrowth5Y=-2.09, revenueGrowthQuarterlyYoy=-10.32, revenueGrowthTTMYoy=-8.18, revenuePerShareAnnual=0.0018, revenuePerShareTTM=0.0018, revenueShareGrowth5Y=-24.42, roa5Y=-2.41, roaRfy=-5.140000000000001, roaTTM=-5.09, roe5Y=-6.65, roeRfy=-16.5, roeTTM=-15.43, roi5Y=-4.42, roiAnnual=-10.05, roiTTM=-9.520000000000001, tangibleBookValuePerShareAnnual=0.0028, tangibleBookValuePerShareQuarterly=0.0008, tbvCagr5Y=-25.89, totalDebt/totalEquityAnnual=0.6418, totalDebt/totalEquityQuarterly=0.6418, yearToDatePriceReturnDaily=0)
[Stage 6:>                                                          (0 + 4) / 4][Stage 6:==============>                                            (1 + 3) / 4][Stage 6:=============================>                             (2 + 2) / 4][Stage 6:============================================>              (3 + 1) / 4]                                                                                ----------------------------------------
Exception occurred during processing of request from ('127.0.0.1', 43672)
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/socketserver.py", line 316, in _handle_request_noblock
    self.process_request(request, client_address)
  File "/usr/local/python/3.10.13/lib/python3.10/socketserver.py", line 347, in process_request
    self.finish_request(request, client_address)
  File "/usr/local/python/3.10.13/lib/python3.10/socketserver.py", line 360, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File "/usr/local/python/3.10.13/lib/python3.10/socketserver.py", line 747, in __init__
    self.handle()
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/accumulators.py", line 295, in handle
    poll(accum_updates)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/accumulators.py", line 267, in poll
    if self.rfile in r and func():
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/accumulators.py", line 271, in accum_updates
    num_updates = read_int(self.rfile)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/serializers.py", line 596, in read_int
    raise EOFError
EOFError
----------------------------------------
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[32m2024-06-15 13:44:10 +0000[0m - dagster - [34mINFO[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - spark_operator.create_stock_tables[stock_AACS] - Pyspark Error: An error occurred while calling o494.count
[32m2024-06-15 13:44:11 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 166558 - spark_operator.create_stock_tables[stock_AACS] - STEP_OUTPUT - Yielded output "metric_and_series" of type "Any". (Type check passed).
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[32m2024-06-15 13:44:15 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 166558 - spark_operator.create_stock_tables[stock_AACS] - STEP_UP_FOR_RETRY - Execution of step "spark_operator.create_stock_tables[stock_AACS]" failed and has requested a retry in 0.2384190540136104 seconds.

dagster._core.execution.plan.utils.RetryRequestedFromPolicy

Stack Trace:
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_plan.py", line 282, in dagster_event_sequence_for_step
    for step_event in check.generator(step_events):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py", line 532, in core_dagster_event_sequence_for_step
    for evt in _type_check_and_store_output(step_context, user_event):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py", line 587, in _type_check_and_store_output
    for evt in _store_output(step_context, step_output_handle, output):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py", line 810, in _store_output
    for elt in iterate_with_context(
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_utils/__init__.py", line 466, in iterate_with_context
    with context_fn():
  File "/usr/local/python/3.10.13/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/utils.py", line 72, in op_execution_error_boundary
    raise RetryRequestedFromPolicy(

The above exception was caused by the following exception:
ConnectionRefusedError: [Errno 111] Connection refused

Stack Trace:
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/utils.py", line 54, in op_execution_error_boundary
    yield
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_utils/__init__.py", line 468, in iterate_with_context
    next_output = next(iterator)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py", line 800, in _gen_fn
    gen_output = output_manager.handle_output(output_context, output.value)
  File "/workspaces/practical-data-engineering/finnhub-batch-stock-pipeline/finnhub_batch_stock_pipeline/iomanagers/staging_s3_parquet_io_manager.py", line 151, in handle_output
    return self.inner_io_manager().handle_output(context, obj)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_utils/cached_method.py", line 104, in _cached_method_wrapper
    result = method(self, *args, **kwargs)
  File "/workspaces/practical-data-engineering/finnhub-batch-stock-pipeline/finnhub_batch_stock_pipeline/iomanagers/staging_s3_parquet_io_manager.py", line 139, in inner_io_manager
    return S3PandasParquetIOInternalManager(
  File "/workspaces/practical-data-engineering/finnhub-batch-stock-pipeline/finnhub_batch_stock_pipeline/iomanagers/staging_s3_parquet_io_manager.py", line 37, in __init__
    self.pyspark = pyspark.spark_session
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster_pyspark/resources.py", line 134, in spark_session
    self._init_session()
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster_pyspark/resources.py", line 130, in _init_session
    self._spark_session = spark_session_from_config(self.spark_config)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster_pyspark/resources.py", line 20, in spark_session_from_config
    return builder.getOrCreate()
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/sql/session.py", line 503, in getOrCreate
    getattr(session._jvm, "SparkSession$"), "MODULE$"
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py", line 1712, in __getattr__
    answer = self._gateway_client.send_command(
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))


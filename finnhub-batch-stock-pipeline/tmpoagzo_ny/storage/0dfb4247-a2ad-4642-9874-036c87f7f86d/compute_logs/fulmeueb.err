[32m2024-06-15 13:48:32 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 192105 - LOGS_CAPTURED - Started capturing logs in process (pid: 192105).
[32m2024-06-15 13:48:32 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 192105 - spark_operator.create_stock_tables[stock_AAIIQ] - STEP_START - Started execution of step "spark_operator.create_stock_tables[stock_AAIIQ]".
[32m2024-06-15 13:48:32 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - spark_operator.create_stock_tables[stock_AAIIQ] - Loading file from: /workspaces/practical-data-engineering/finnhub-batch-stock-pipeline/tmpoagzo_ny/storage/0dfb4247-a2ad-4642-9874-036c87f7f86d/spark_operator.stock_tables/result/stock_AAIIQ using PickledObjectFilesystemIOManager...
[32m2024-06-15 13:48:32 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 192105 - spark_operator.create_stock_tables[stock_AAIIQ] - LOADED_INPUT - Loaded input "input_fn" using input manager "io_manager", from output "result" of step "spark_operator.stock_tables"
[32m2024-06-15 13:48:32 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 192105 - spark_operator.create_stock_tables[stock_AAIIQ] - STEP_INPUT - Got input "input_fn" of type "Any". (Type check passed).
24/06/15 13:48:46 WARN Utils: Your hostname, codespaces-e7c8c2 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)
24/06/15 13:48:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
24/06/15 13:48:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/06/15 13:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
24/06/15 13:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
24/06/15 13:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
24/06/15 13:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
24/06/15 13:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[32m2024-06-15 13:49:04 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 192105 - spark_operator.create_stock_tables[stock_AAIIQ] - STEP_UP_FOR_RETRY - Execution of step "spark_operator.create_stock_tables[stock_AAIIQ]" failed and has requested a retry in 0.3287153837612444 seconds.

dagster._core.definitions.events.RetryRequested

Stack Trace:
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_plan.py", line 282, in dagster_event_sequence_for_step
    for step_event in check.generator(step_events):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py", line 525, in core_dagster_event_sequence_for_step
    for user_event in _step_output_error_checked_user_event_sequence(
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py", line 203, in _step_output_error_checked_user_event_sequence
    for user_event in user_event_sequence:
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py", line 100, in _process_asset_results_to_events
    for user_event in user_event_sequence:
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/compute.py", line 208, in execute_core_compute
    for step_output in _yield_compute_results(step_context, inputs, compute_fn, compute_context):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/compute.py", line 177, in _yield_compute_results
    for event in iterate_with_context(
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_utils/__init__.py", line 466, in iterate_with_context
    with context_fn():
  File "/usr/local/python/3.10.13/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/utils.py", line 94, in op_execution_error_boundary
    raise RetryRequested(

The above exception was caused by the following exception:
dagster._core.errors.DagsterExecutionInterruptedError

Stack Trace:
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/utils.py", line 54, in op_execution_error_boundary
    yield
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_utils/__init__.py", line 468, in iterate_with_context
    next_output = next(iterator)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/compute_generator.py", line 141, in _coerce_op_compute_fn_to_iterator
    result = invoke_compute_fn(
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_core/execution/plan/compute_generator.py", line 129, in invoke_compute_fn
    return fn(context, **args_to_pass) if context_arg_provided else fn(**args_to_pass)
  File "/workspaces/practical-data-engineering/finnhub-batch-stock-pipeline/finnhub_batch_stock_pipeline/assets/spark_transformations.py", line 120, in create_stock_tables
    spark = pyspark.spark_session
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster_pyspark/resources.py", line 134, in spark_session
    self._init_session()
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster_pyspark/resources.py", line 130, in _init_session
    self._spark_session = spark_session_from_config(self.spark_config)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster_pyspark/resources.py", line 20, in spark_session_from_config
    return builder.getOrCreate()
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/sql/session.py", line 497, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/context.py", line 515, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/context.py", line 203, in __init__
    self._do_init(
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/context.py", line 296, in _do_init
    self._jsc = jsc or self._initialize_context(self._conf._jconf)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/context.py", line 421, in _initialize_context
    return self._jvm.JavaSparkContext(jconf)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py", line 1586, in __call__
    answer = self._gateway_client.send_command(command)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/python/3.10.13/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/dagster/_utils/interrupts.py", line 82, in _new_signal_handler
    raise error_cls()


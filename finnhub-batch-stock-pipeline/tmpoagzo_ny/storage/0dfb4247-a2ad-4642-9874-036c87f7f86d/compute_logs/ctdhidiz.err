[32m2024-06-15 13:42:47 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 166554 - LOGS_CAPTURED - Started capturing logs in process (pid: 166554).
[32m2024-06-15 13:42:48 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 166554 - spark_operator.create_stock_tables[stock_AABB] - STEP_START - Started execution of step "spark_operator.create_stock_tables[stock_AABB]".
[32m2024-06-15 13:42:48 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - spark_operator.create_stock_tables[stock_AABB] - Loading file from: /workspaces/practical-data-engineering/finnhub-batch-stock-pipeline/tmpoagzo_ny/storage/0dfb4247-a2ad-4642-9874-036c87f7f86d/spark_operator.stock_tables/result/stock_AABB using PickledObjectFilesystemIOManager...
[32m2024-06-15 13:42:48 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 166554 - spark_operator.create_stock_tables[stock_AABB] - LOADED_INPUT - Loaded input "input_fn" using input manager "io_manager", from output "result" of step "spark_operator.stock_tables"
[32m2024-06-15 13:42:48 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 166554 - spark_operator.create_stock_tables[stock_AABB] - STEP_INPUT - Got input "input_fn" of type "Any". (Type check passed).
24/06/15 13:42:56 WARN Utils: Your hostname, codespaces-e7c8c2 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)
24/06/15 13:42:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
24/06/15 13:42:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/06/15 13:43:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
24/06/15 13:43:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
24/06/15 13:43:22 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors
[Stage 0:>                                                          (0 + 0) / 4][Stage 0:>                                                          (0 + 4) / 4][Stage 0:============================================>              (3 + 1) / 4]                                                                                [Stage 1:>                                                          (0 + 4) / 4][Stage 1:=============================>                             (2 + 2) / 4]                                                                                [Stage 4:>                                                          (0 + 1) / 1]                                                                                [Stage 5:>                                                          (0 + 3) / 3]                                                                                [32m2024-06-15 13:43:58 +0000[0m - dagster - [34mINFO[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - spark_operator.create_stock_tables[stock_AABB] - Row(10DayAverageTradingVolume=7.39613, 13WeekPriceReturnDaily=-37.2197, 26WeekPriceReturnDaily=-30.6931, 3MonthADReturnStd=72.77, 3MonthAverageTradingVolume=7.76496, 52WeekHigh=0.0379, 52WeekHighDate='2024-02-29', 52WeekLow=0.0114, 52WeekLowDate='2023-10-16', 52WeekPriceReturnDaily=-49.0909, 5DayPriceReturnDaily=6.0606, beta=-0.9945878, monthToDatePriceReturnDaily=-10.2564, priceRelativeToS&P50013Week=-42.3518, priceRelativeToS&P50026Week=-49.0725, priceRelativeToS&P5004Week=-29.3043, priceRelativeToS&P50052Week=-73.1702, priceRelativeToS&P500Ytd=-43.0595, yearToDatePriceReturnDaily=-28.934)
[Stage 6:>                                                          (0 + 4) / 4][Stage 6:=============================>                             (2 + 2) / 4][Stage 7:>                                                          (0 + 4) / 4][Stage 7:=============================>                             (2 + 2) / 4][32m2024-06-15 13:44:04 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 166554 - spark_operator.create_stock_tables[stock_AABB] - STEP_OUTPUT - Yielded output "metric_and_series" of type "Any". (Type check passed).
[32m2024-06-15 13:44:07 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - spark_operator.create_stock_tables[stock_AABB] - Writing S3 object at: s3a://dagster-api/staging/2024-06-15/spark_operator.create_stock_tables[stock_AABB]/metric_and_series
[32m2024-06-15 13:44:07 +0000[0m - dagster - [34mWARNING[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - spark_operator.create_stock_tables[stock_AABB] - Removing existing S3 object: staging/2024-06-15/spark_operator.create_stock_tables[stock_AABB]/metric_and_series
24/06/15 13:44:09 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[Stage 10:>                                                         (0 + 4) / 4][Stage 10:=============================>                            (2 + 2) / 4]                                                                                [Stage 11:>                                                         (0 + 1) / 1]                                                                                24/06/15 13:45:00 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 12:>                                                         (0 + 1) / 1][Stage 14:>                                                        (0 + 4) / 50][Stage 14:====>                                                    (4 + 4) / 50][Stage 14:=========>                                               (8 + 4) / 50][Stage 14:=============>                                          (12 + 4) / 50][Stage 14:===============>                                        (14 + 4) / 50][Stage 14:=================>                                      (16 + 4) / 50][Stage 14:====================>                                   (18 + 4) / 50][Stage 14:======================>                                 (20 + 4) / 50][Stage 14:==========================>                             (24 + 4) / 50][Stage 14:===============================>                        (28 + 4) / 50][Stage 14:=================================>                      (30 + 4) / 50][Stage 14:===================================>                    (32 + 4) / 50][Stage 14:========================================>               (36 + 4) / 50][Stage 14:===========================================>            (39 + 4) / 50][Stage 14:============================================>           (40 + 4) / 50][Stage 14:===============================================>        (42 + 4) / 50][Stage 14:==================================================>     (45 + 4) / 50][Stage 14:===================================================>    (46 + 4) / 50]                                                                                [32m2024-06-15 13:45:14 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 166554 - spark_operator.create_stock_tables[stock_AABB] - HANDLED_OUTPUT - Handled output "metric_and_series" using IO manager "s3_prqt_io_manager"
[32m2024-06-15 13:45:14 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 0dfb4247-a2ad-4642-9874-036c87f7f86d - 166554 - spark_operator.create_stock_tables[stock_AABB] - STEP_SUCCESS - Finished execution of step "spark_operator.create_stock_tables[stock_AABB]" in 2m26s.

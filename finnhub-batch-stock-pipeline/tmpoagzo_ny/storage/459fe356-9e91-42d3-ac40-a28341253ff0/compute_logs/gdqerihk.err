[32m2024-06-15 13:21:27 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - 130420 - LOGS_CAPTURED - Started capturing logs in process (pid: 130420).
[32m2024-06-15 13:21:27 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - 130420 - spark_operator.merge_and_analyze - STEP_START - Started execution of step "spark_operator.merge_and_analyze".
24/06/15 13:21:28 WARN Utils: Your hostname, codespaces-e7c8c2 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)
24/06/15 13:21:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
24/06/15 13:21:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[32m2024-06-15 13:21:31 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - spark_operator.merge_and_analyze - Loading S3 object from: s3a://dagster-api/staging/2024-06-15/spark_operator.create_stock_tables[stock_ABCL]/metric_and_series
24/06/15 13:21:32 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[32m2024-06-15 13:21:37 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - 130420 - spark_operator.merge_and_analyze - LOADED_INPUT - Loaded input "df_list" using input manager "s3_prqt_io_manager", from output "metric_and_series" of step "spark_operator.create_stock_tables[stock_ABCL]"
[32m2024-06-15 13:21:37 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - 130420 - spark_operator.merge_and_analyze - STEP_INPUT - Got input "df_list" of type "Any". (Type check passed).
24/06/15 13:21:38 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 3:>                                                         (0 + 4) / 50][Stage 3:====>                                                     (4 + 4) / 50][Stage 3:=========>                                                (8 + 4) / 50][Stage 3:=============>                                           (12 + 4) / 50][Stage 3:====================>                                    (18 + 4) / 50][Stage 3:===========================>                             (24 + 4) / 50]24/06/15 13:21:41 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors
[Stage 3:===============================>                         (28 + 4) / 50][Stage 3:=========================================>               (36 + 4) / 50][Stage 3:===============================================>         (42 + 4) / 50]                                                                                [32m2024-06-15 13:21:44 +0000[0m - dagster - [34mINFO[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - spark_operator.merge_and_analyze - srsmetric_col_len:    114
[32m2024-06-15 13:21:44 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - 130420 - spark_operator.merge_and_analyze - STEP_OUTPUT - Yielded output "metric" of type "Any". (Type check passed).
[32m2024-06-15 13:21:44 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - spark_operator.merge_and_analyze - Writing S3 object at: s3a://dagster-api/staging/2024-06-15/metric
[32m2024-06-15 13:21:44 +0000[0m - dagster - [34mWARNING[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - spark_operator.merge_and_analyze - Removing existing S3 object: staging/2024-06-15/metric
[Stage 13:=================>                                      (16 + 4) / 50][Stage 13:==================================>                     (31 + 4) / 50][Stage 13:===================================================>    (46 + 4) / 50]                                                                                [Stage 14:>                                                         (0 + 4) / 6][Stage 14:=============================>                            (3 + 3) / 6][Stage 14:================================================>         (5 + 1) / 6]                                                                                [Stage 18:========================>                               (22 + 4) / 50][Stage 18:================================>                       (29 + 4) / 50][Stage 18:======================================>                 (34 + 4) / 50][Stage 18:===============================================>        (42 + 4) / 50][Stage 18:=====================================================>  (48 + 2) / 50]                                                                                [32m2024-06-15 13:21:51 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - 130420 - spark_operator.merge_and_analyze - ASSET_MATERIALIZATION - Materialized value metric.
[32m2024-06-15 13:21:51 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - 130420 - spark_operator.merge_and_analyze - HANDLED_OUTPUT - Handled output "metric" using IO manager "s3_prqt_io_manager"
[32m2024-06-15 13:21:51 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - 130420 - spark_operator.merge_and_analyze - STEP_OUTPUT - Yielded output "series" of type "Any". (Type check passed).
[32m2024-06-15 13:21:51 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - spark_operator.merge_and_analyze - Writing S3 object at: s3a://dagster-api/staging/2024-06-15/series
[32m2024-06-15 13:21:51 +0000[0m - dagster - [34mWARNING[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - spark_operator.merge_and_analyze - Removing existing S3 object: staging/2024-06-15/series
[Stage 23:============================================>           (40 + 4) / 50]                                                                                [Stage 24:=========>                                                (1 + 4) / 6][Stage 24:================================================>         (5 + 1) / 6]                                                                                [Stage 28:=============================>                          (26 + 4) / 50][Stage 28:========================================>               (36 + 4) / 50][Stage 28:===================================================>    (46 + 4) / 50]                                                                                [32m2024-06-15 13:21:56 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - 130420 - spark_operator.merge_and_analyze - ASSET_MATERIALIZATION - Materialized value series.
[32m2024-06-15 13:21:56 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - 130420 - spark_operator.merge_and_analyze - HANDLED_OUTPUT - Handled output "series" using IO manager "s3_prqt_io_manager"
[32m2024-06-15 13:21:56 +0000[0m - dagster - [34mDEBUG[0m - __ASSET_JOB - 459fe356-9e91-42d3-ac40-a28341253ff0 - 130420 - spark_operator.merge_and_analyze - STEP_SUCCESS - Finished execution of step "spark_operator.merge_and_analyze" in 28.63s.
